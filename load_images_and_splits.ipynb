{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all images from all cameras to one folder\n",
    "def copy_images_from_dataset(dataset_path, dst_path):\n",
    "    if not os.path.isdir(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "    for car_name in tqdm(os.listdir(dataset_path), desc='Copying images'):\n",
    "        car_path = osp.join(dataset_path, car_name)\n",
    "        for camera_name in os.listdir(car_path):\n",
    "            cur_camera_path = osp.join(car_path, camera_name)\n",
    "            for image_name in os.listdir(cur_camera_path):\n",
    "                shutil.copy(osp.join(cur_camera_path, image_name), dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy_images_from_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mveri\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m dst_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mveri_images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcopy_images_from_dataset\u001b[49m(dataset_path, dst_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy_images_from_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_path = osp.join('..', '..', 'veri')\n",
    "dst_path = osp.join('dataset', 'veri_images')\n",
    "copy_images_from_dataset(dataset_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0247_c002_00089940_0.jpg',\n",
       " '0691_c009_00089940_0.jpg',\n",
       " '0691_c009_00089945_0.jpg',\n",
       " '0247_c002_00089950_0.jpg',\n",
       " '0691_c009_00089960_0.jpg',\n",
       " '0247_c002_00089960_0.jpg',\n",
       " '0326_c009_00089965_0.jpg',\n",
       " '0247_c002_00089970_0.jpg',\n",
       " '0326_c009_00089970_0.jpg',\n",
       " '0326_c009_00089975_0.jpg',\n",
       " '0247_c002_00089980_0.jpg',\n",
       " '0326_c009_00089980_0.jpg',\n",
       " '0326_c009_00089985_0.jpg',\n",
       " '0326_c009_00089990_0.jpg',\n",
       " '0247_c002_00089990_0.jpg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(os.listdir(dst_path), key=lambda s: int(s.split('_')[2]))[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning images: 100%|██████████| 49357/49357 [00:00<00:00, 642397.15it/s]\n",
      "Making split: 100%|██████████| 776/776 [00:00<00:00, 95963.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35737, 11936, 1684)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder = dst_path\n",
    "\n",
    "# Dict to save info about all images\n",
    "data = defaultdict(lambda: defaultdict(list))\n",
    "for filename in tqdm(os.listdir(image_folder), desc='Scanning images'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        car_id, camera_id, image_id, _ = filename.split('_')\n",
    "        data[car_id][camera_id].append(filename)\n",
    "\n",
    "# Split images into train and test (query + galley)\n",
    "train_data: list[str] = []\n",
    "query_data: list[str] = []\n",
    "gallery_data: list[str] = []\n",
    "random.seed(0)\n",
    "for car_id, cameras in tqdm(data.items(), desc='Making split'):\n",
    "    for camera_id, images in cameras.items():\n",
    "        # if at least two images for this camera are present, add some of them to test\n",
    "        if len(images) >= 2:\n",
    "            # 1 in 4 chance to add first image to query, 3 in 4 to add it to gallery\n",
    "            if random.randint(0, 3) == 0:\n",
    "                query_data.append(images[0])\n",
    "            else:\n",
    "                gallery_data.append(images[0])\n",
    "            # second image always goes to gallery so it always has images of cars that are in query\n",
    "            gallery_data.append(images[1])\n",
    "            # the rest goes to train\n",
    "            train_data.extend(images[2:])\n",
    "        else:\n",
    "            train_data.extend(images)\n",
    "\n",
    "len(train_data), len(gallery_data), len(query_data)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save split image names to respective files\n",
    "with open(os.path.join('dataset', 'VeRi', 'name_train.txt'), 'w+') as f:\n",
    "    f.write('\\n'.join(train_data))\n",
    "with open(os.path.join('dataset', 'VeRi', 'name_test.txt'), 'w+') as f:\n",
    "    f.write('\\n'.join(gallery_data))\n",
    "with open(os.path.join('dataset', 'VeRi', 'name_query.txt'), 'w+') as f:\n",
    "    f.write('\\n'.join(query_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(gallery_data).isdisjoint(query_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49357"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = train_data + query_data + gallery_data\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49356, 49294)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform keypoin_orient (viewpoints) into one file (since our split does not match the one in the\n",
    "# original paper and therefore their train/test dplit does not make sence for us)\n",
    "df1 = pd.read_csv(os.path.join('dataset', 'VeRi', 'keypoint_orient_train.txt'), sep=' ', header = None)\n",
    "df2 = pd.read_csv(os.path.join('dataset', 'VeRi', 'keypoint_orient_test.txt'), sep=' ', header = None)\n",
    "df = pd.concat([df1, df2])\n",
    "all_keys = np.unique(df[0])\n",
    "len(all_keys), len(np.intersect1d(all_images, all_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow there are 62 images missing from viewpoint files... How can this be?\n",
    "\n",
    "Anyway, save what we have to one file (the model has native missing viewpoint support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49357/49357 [05:29<00:00, 149.60it/s]\n"
     ]
    }
   ],
   "source": [
    "all_views = {img_name: df.loc[df[0] == img_name, 1:].to_numpy() for img_name in tqdm(all_images) if img_name in all_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_views = {img_name: arr[0].tolist() for img_name, arr in all_views.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('dataset', 'VeRi', 'viewpoints.txt'), \"w\") as f: \n",
    "    json.dump(all_views, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 67,\n",
       " 77,\n",
       " 117,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 29,\n",
       " 33,\n",
       " -1,\n",
       " -1,\n",
       " 114,\n",
       " 12,\n",
       " 63,\n",
       " 14,\n",
       " 107,\n",
       " 46,\n",
       " 177,\n",
       " 44,\n",
       " 113,\n",
       " 89,\n",
       " 196,\n",
       " 92,\n",
       " 155,\n",
       " 88,\n",
       " 156,\n",
       " 123,\n",
       " 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join('dataset', 'VeRi', 'viewpoints.txt'), 'r') as f:\n",
    "    viewpoints = json.load(f)\n",
    "\n",
    "viewpoints['0208_c016_00038985_0.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/serg_fedchn/Homework/6_semester/НИР/object-reidentification/baseline_reworked'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "%ls ../../veri_splits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "true_splits_path = '../../veri_splits' \n",
    "with open(osp.join(true_splits_path, 'veri_train_list.txt'), 'r') as f:\n",
    "    train_list = set(map(lambda s: s.strip().split()[0], f.readlines()))\n",
    "with open(osp.join(true_splits_path, 'veri_test_list.txt'), 'r') as f:\n",
    "    test_list = set(map(lambda s: s.strip(), f.readlines()))\n",
    "with open(osp.join(true_splits_path, 'veri_query_list.txt'), 'r') as f:\n",
    "    query_list = set(map(lambda s: s.strip(), f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37778, 11579, 1678)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list), len(test_list),  len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49357"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list) + len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_list = test_list - query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1678, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_list.intersection(test_list)), len(train_list.intersection(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/serg_fedchn/Homework/6_semester/НИР/object-reidentification/baseline_reworked'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join('dataset', 'VeRi', 'original_veri_train_list.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(train_list))\n",
    "with open(osp.join('dataset', 'VeRi', 'original_veri_test_list.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_list))\n",
    "with open(osp.join('dataset', 'VeRi', 'original_veri_query_list.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(query_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944449"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset/VeRi/original_veri_train_list.txt', 'r') as f:\n",
    "    lines = f.read()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
